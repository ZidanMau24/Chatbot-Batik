{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZidanMau24/Chatbot-Batik/blob/main/QA_System_Batik.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usduAw6FdJxE"
      },
      "source": [
        "<img src=\"https://2.bp.blogspot.com/-066qpJs0Ttc/WiYPXGNYEYI/AAAAAAAAFu8/XbOaf7DqfDMM9truu3DkrkIGfRgP4zBzgCLcBGAs/s1600/udinus.jpg\"  width=\"200\">\n",
        "\n",
        "# AMS - QA System\n",
        "\n",
        "oleh: Dr. Eng. Farrikh Alzami M.Kom; Abu Salam, M.Kom\n",
        "\n",
        "disini kita akan belajar menggunakan modul:\n",
        "\n",
        "dataset: (COVID19_wikipedia article.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lkfb64rfuca",
        "outputId": "a366cb29-8b60-4f85-af39-bc31fa7f19c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')\n",
        " import sys\n",
        " sys.path.append('/content/drive/My Drive/Colab Notebooks/QNA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91b3-Eo6uIev",
        "outputId": "8f0e8993-68c5-4909-ff34-b58b77700e88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import pathlib\n",
        "pathlib.Path().resolve()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SGK_isHMdVd",
        "outputId": "d7199dc7-5247-405e-c7d9-0922dc6dcdcd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sastrawi in /usr/local/lib/python3.9/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ArZ7hoVk0-Z",
        "outputId": "83737393-8127-47de-e5aa-7dd15569488e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "#importing the required packages\n",
        "import nltk\n",
        "#### Run it, if before never download nltk library\n",
        "# nltk.download('all')\n",
        "import re\n",
        "import string\n",
        "import gensim \n",
        "\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "from nltk import pos_tag,word_tokenize,ne_chunk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pandas import DataFrame\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "from nltk.corpus import wordnet,stopwords\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX8Y2H02dwAC"
      },
      "source": [
        "**Uploading the Reference Article**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ce46MqrGf_Z_"
      },
      "outputs": [],
      "source": [
        "sample = '/content/drive/My Drive/Colab Notebooks/QNA/batik revisi.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "N3kTgL0Qk6tn"
      },
      "outputs": [],
      "source": [
        " #from google.colab import files\n",
        " #uploaded=files.upload()\n",
        "sample = open(\"/content/drive/My Drive/Colab Notebooks/QNA/batik revisi.txt\", \"r\") \n",
        "\n",
        "s = sample.read() \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "0nUC_lZHgdxP",
        "outputId": "4d2ac231-ba26-436c-b09b-022d75e0c896"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Batik adalah seni membuat kain dengan teknik pewarnaan khusus yang berasal dari Indonesia. Motif-motif batik dihasilkan dengan mengecat atau menorehkan lilin pada kain yang kemudian dicelupkan ke dalam pewarna.\\n\\nProses pembuatan batik biasanya dimulai dengan menggambar desain pada kain dengan menggunakan pensil. Kemudian, lilin dipanaskan hingga meleleh dan ditempelkan pada bagian kain yang tidak ingin diwarnai. Setelah itu, kain dicelupkan ke dalam pewarna, di mana warna hanya menempel pada bagian kain yang tidak tertutup lilin. Setelah kain diwarnai, lilin dihilangkan dengan memanaskannya atau dengan mencucinya.\\n\\nBatik memiliki berbagai macam motif dan warna yang berbeda, yang sering kali merefleksikan budaya dan kearifan lokal. Batik juga digunakan sebagai pakaian formal di Indonesia dan diakui sebagai warisan budaya dunia oleh UNESCO pada tahun 2009.\\n\\nSekarang, batik bukan hanya digunakan sebagai kain, tetapi juga diterapkan pada berbagai macam produk seperti tas, sepatu, baju, dan aksesoris. Batik juga menjadi salah satu produk ekspor penting Indonesia.\\nBatik telah menjadi bagian penting dari kehidupan masyarakat Indonesia selama berabad-abad. Batik awalnya digunakan sebagai kain untuk pakaian sehari-hari, namun seiring berjalannya waktu, batik juga menjadi simbol status sosial dan dianggap sebagai warisan budaya yang berharga.\\n\\nSetiap daerah di Indonesia memiliki motif batik yang khas dan unik. Beberapa contoh motif batik yang terkenal adalah Batik Parang dari Yogyakarta, Batik Lasem dari Jawa Tengah, dan Batik Tulis dari Solo.\\n\\nPada awalnya, batik dibuat dengan cara yang sangat tradisional dan memakan waktu yang lama. Namun, seiring perkembangan teknologi, proses pembuatan batik menjadi lebih mudah dan cepat. Meskipun demikian, batik yang dibuat dengan teknik tradisional masih dianggap lebih berharga dan memiliki nilai seni yang lebih tinggi.\\n\\nSaat ini, batik telah menjadi industri yang berkembang pesat di Indonesia, dengan banyak perusahaan batik yang menghasilkan produk berkualitas tinggi dan inovatif. Selain itu, batik juga terus menjadi bagian penting dari budaya Indonesia dan dihargai oleh banyak orang di seluruh dunia.\\n\\nAda banyak jenis batik yang berasal dari berbagai daerah di Indonesia. Berikut adalah beberapa jenis batik yang terkenal:\\n\\nBatik Parang - berasal dari Yogyakarta, memiliki motif geometris yang melambangkan energi positif.\\n\\nBatik Kawung - berasal dari Jawa Tengah, memiliki pola bulat-bulat yang menyimbolkan cakram atau roda dalam kebudayaan Jawa.\\n\\nBatik Megamendung - berasal dari Cirebon, memiliki motif awan yang dilukiskan dalam bentuk melengkung dan diisi dengan warna biru.\\n\\nBatik Truntum - berasal dari Solo, memiliki pola berupa guratan-guratan yang terlihat seperti pola hati.\\n\\nBatik Lereng - berasal dari Pekalongan, memiliki pola geometris yang dibuat dengan teknik cetak.\\n\\nBatik Lasem - berasal dari Jawa Tengah, memiliki pola yang unik dan cerah dengan warna-warna cerah.\\n\\nBatik Tulis - merupakan jenis batik yang dibuat secara tradisional dengan teknik menulis atau menorehkan motif pada kain menggunakan lilin cair.\\n\\nBatik Cap - merupakan jenis batik yang dibuat dengan teknik cap atau stempel, sehingga lebih cepat dan mudah dibuat.\\n\\nBatik Printing - merupakan jenis batik yang dibuat dengan menggunakan teknik mencetak pada kain yang telah diberi lilin cair atau terlebih dahulu dicap.\\n\\nSetiap jenis batik memiliki keunikan dan nilai seni yang berbeda-beda, tergantung pada motif, warna, dan teknik pembuatannya.\\n\\nUntuk pernikahan, biasanya batik yang digunakan adalah batik dengan pola yang lebih formal dan elegan. Berikut ini beberapa jenis batik yang cocok untuk digunakan pada acara pernikahan:\\n\\nBatik Prada - Batik Prada atau Batik Pekalongan merupakan batik dengan pola yang elegan dan bervariasi. Batik ini umumnya menggunakan warna-warna cerah, seperti warna merah, hijau, biru, atau kuning, dan memiliki pola-pola bunga yang indah.\\n\\nBatik Kawung - Batik Kawung juga cocok untuk dipakai pada acara pernikahan. Batik ini memiliki pola bulat-bulat yang menyimbolkan cakram atau roda dalam kebudayaan Jawa, dan terlihat sangat elegan dan formal.\\n\\nBatik Mega Mendung - Batik Mega Mendung berasal dari Cirebon, dan biasanya memiliki pola awan yang melengkung dengan warna biru dan putih. Batik ini sangat cocok untuk acara pernikahan yang bertema tradisional atau Jawa.\\n\\nBatik Sidomukti - Batik Sidomukti memiliki pola yang khas, dengan bentuk unik yang menyerupai jamur. Batik ini biasanya menggunakan warna-warna cerah, seperti merah, hijau, dan kuning, sehingga sangat cocok untuk dipakai pada acara pernikahan.\\n\\nBatik Truntum - Batik Truntum memiliki pola berupa guratan-guratan yang terlihat seperti pola hati, dan biasanya menggunakan warna-warna cerah seperti merah atau kuning. Batik ini juga terlihat sangat elegan dan formal, sehingga cocok untuk acara pernikahan.\\n\\nPilihan batik untuk acara pernikahan sangat bergantung pada selera dan tema yang diinginkan. Namun, batik dengan pola-pola yang lebih formal dan elegan umumnya lebih cocok untuk acara pernikahan daripada batik dengan pola yang lebih casual.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCo5W9nCd9Pl"
      },
      "source": [
        "**Text Preprocessing Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zNjJvOtSk7bs"
      },
      "outputs": [],
      "source": [
        "def stem_sentence(sentence):\n",
        "  words=word_tokenize(sentence)\n",
        "  #lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  new_words=[]\n",
        "  for i in words:\n",
        "    new_words.append(stemmer.stem(i))\n",
        "    new_words.append(\" \")\n",
        "  return \"\".join(new_words)  \n",
        "  \n",
        "\n",
        "\n",
        "def clean_sentence(sentence, stopwords=True):\n",
        "    \n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
        "    \n",
        "    \n",
        "    \n",
        "    if stopwords:\n",
        "         sentence = remove_stopwords(sentence)\n",
        "    \n",
        "   \n",
        "    \n",
        "    return sentence\n",
        "def get_cleaned_sentences(sents,stopwords=True):    \n",
        "    \n",
        "    cleaned_sentences=[]\n",
        "\n",
        "    for i in sents:\n",
        "        \n",
        "        cleaning=clean_sentence(i,stopwords)\n",
        "        cleaned=stem_sentence(cleaning)\n",
        "        cleaned_sentences.append(cleaned)\n",
        "    return cleaned_sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKROOQz6edpa"
      },
      "source": [
        "**create_document_term_matrix: Function to create TF - IDF Matrix**\n",
        "\n",
        "\n",
        "**calculate_cosine_similarity: Function to return the top 3 sentences based on cosine similarity between TF-IDF scores**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrfY_71UeRcg"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RFQbswivlAlp"
      },
      "outputs": [],
      "source": [
        "def create_document_term_matrix(sen,vectorizer):\n",
        "  doc_term_matrix=vectorizer.fit_transform(sen)\n",
        "  return DataFrame(doc_term_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPdXonU2e9Ew"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "28eA0yL0lFL7"
      },
      "outputs": [],
      "source": [
        "def calculate_cosine_similarity(df_list,sentences,question):\n",
        "  a=[]\n",
        "  for i in range(len(df_list)-1):\n",
        "    sim=1 - spatial.distance.cosine(df_list[i], question)\n",
        "    t=(sim,sentences[i])\n",
        "    a.append(t)\n",
        "  a.sort(reverse=True)\n",
        "  n=[]\n",
        "  for i in range(3):\n",
        "    n.append(a[i][1])\n",
        "    #print(\"*\",n[i])\n",
        "   \n",
        "    \n",
        "  return n  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA81IVTvdGz6"
      },
      "source": [
        "**Function to classify questions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tRYoXgLYlNSD"
      },
      "outputs": [],
      "source": [
        "def questiontype( question):\n",
        "        questiontags = ['WP','WDT','WP$','WRB']\n",
        "        question_POS = pos_tag(word_tokenize(question.lower()))\n",
        "        \n",
        "        question_Tags=[]\n",
        "        for token in question_POS:\n",
        "            if token[1] in questiontags:\n",
        "              question_Tags.append(token)\n",
        "                \n",
        "                \n",
        "        if len(question_Tags)==1 and question_Tags[0][0]!= 'apa' :\n",
        "          return True\n",
        "        else:\n",
        "          return False  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKwWilpbfigw"
      },
      "source": [
        "**Function to find the most relevant sentence using n-gram similarity **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8Cli_jJ0lONW"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "def n_gram_similarity(question,n):\n",
        "  q=list(ngrams(word_tokenize(question.lower()),2))\n",
        "  a=0\n",
        "  b=0\n",
        "  c=0\n",
        "  t=[]\n",
        "  for i in q:\n",
        "    if i in list(ngrams(word_tokenize(n[0].lower()),2)):\n",
        "      a=a+1\n",
        "  for i in q:\n",
        "    if i in list(ngrams(word_tokenize(n[1].lower()),2)):\n",
        "      b=b+1\n",
        "  for i in q:\n",
        "    if i in list(ngrams(word_tokenize(n[2].lower()),2)):\n",
        "      c=c+1        \n",
        "  d=max(a,b,c)\n",
        "  if a == d:\n",
        "    t.append(n[0])\n",
        "  if b == d:\n",
        "    t.append(n[1]) \n",
        "  if c ==d:\n",
        "    t.append(n[2])\n",
        "  print()  \n",
        "  print(\"Selected Sentence:\",t[0])  \n",
        "  return t  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaFGtZwff2Cr"
      },
      "source": [
        "**answertype:**Determines the type of entity that has to be returned, performs entity ranking if multiple entities a present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xKBe8u5slTdA"
      },
      "outputs": [],
      "source": [
        "def answertype(question):\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "  if (questiontype(question)):\n",
        "    t='DESCRIPTIVE'\n",
        "    flag=0\n",
        "    word=word_tokenize(question.lower())\n",
        "  \n",
        "    if 'siapa' in word:\n",
        "      t='PERSON'\n",
        "    elif 'dimana' in word:\n",
        "      t='GPE'\n",
        "    elif 'berapa' in word and 'seberapa' in word and  'lama' in word or 'durasi' in word or 'panjang' in word or 'hari'in word or 'tahun' in word or'bulan' in word:\n",
        "      t='DATE' \n",
        "    elif 'berapa' in word and 'banyak' in word :\n",
        "       t = 'CARDINAL'  \n",
        "    elif 'berapa' in word  or 'umur' in word or 'lama' in word or 'durasi' in word  or 'lama' in word or 'panjang' in word:\n",
        "      t='DATE'\n",
        "    elif 'berapa' in word  and 'panjang' in word or 'sering' or 'umur' in word or 'tahun' in word:\n",
        "      t='DATE' \n",
        "    elif 'apa' in word and 'waktu' in word or 'durasi' in word or 'periode' in  'kata'  :\n",
        "      t='DATE' \n",
        "    i=len(df_list)-1  \n",
        "    n=calculate_cosine_similarity(df_list, sentences,df_list[i])\n",
        "    n=n_gram_similarity(question,n)\n",
        "    #print(\"Most relevant sentence\", n[0])\n",
        "    #print(\"ANSWER TYPE:\",t)\n",
        "    key = n[0]\n",
        "    spdoc = nlp(key)\n",
        "    entity_type=[]\n",
        "    for ent in spdoc.ents:\n",
        "       if ent.label_ == t:\n",
        "          entity_type.append(ent.text)\n",
        "    if len(entity_type) == 1:\n",
        "      #print(\"ANSWER TYPE:\", t)\n",
        "      print(\"ANSWER:\", entity_type[0])  \n",
        "    if len(entity_type) == 0:\n",
        "      #print(\"ANSWER TYPE:\", t) \n",
        "      print(n[0])\n",
        "    if len(entity_type) > 1:\n",
        "      #print(\"Answer Type:\",t)  \n",
        "      key_question = question\n",
        "      q=[]\n",
        "      spdoc = nlp(key_question)\n",
        "      for ent in spdoc:\n",
        "        if ent.pos_ == 'NOUN' or ent.pos_ =='ADJ' :\n",
        "          q.append(ent.text)\n",
        "  \n",
        "      key_answer = n[0]\n",
        "      a = []\n",
        "      spd = nlp(key)\n",
        "      for ent in spd:\n",
        "        if ent.pos_ == 'NOUN'or ent.pos_ =='ADJ' :\n",
        "          a.append(ent.text)\n",
        "  #s=[sentence.index(i) for i in t]\n",
        "      s=[]\n",
        "      w=[]\n",
        "      for i in entity_type:\n",
        "       s.append(n[0].index(i))\n",
        "      for i in range(len(s)):\n",
        "        w.append(0)\n",
        "\n",
        "    \n",
        "      for i in q:\n",
        "        try:\n",
        "           factor= n[0].index(i)\n",
        "           for j in range(len(s)):\n",
        "              w[j]=w[j]+(abs(s[j]-factor))\n",
        "        except:\n",
        "           continue    \n",
        "      m=min(w)\n",
        "      u=[]\n",
        "      for i in range(len(s)):\n",
        "        if w[i] == m:\n",
        "           #print(entity_type[i])\n",
        "           u.append(entity_type[i])\n",
        "      print(\"ANSWER:\",u[0])     \n",
        "      \n",
        "\n",
        "  \n",
        "\n",
        "    \n",
        "  else:\n",
        "    t='DESCRIPTIVE'\n",
        "    #print(\"ANSWER TYPE:\",t)\n",
        "    i=len(df_list)-1  \n",
        "    n=calculate_cosine_similarity(df_list, sentences,df_list[i])\n",
        "    #n = n_gram_similarity(question, n)\n",
        "    for j in n:\n",
        "         print(j) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "m1r9nOcqlezX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c801729-063d-4f38-fa1c-7834241e5416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION: apa itu seni batik\n",
            "Batik adalah seni membuat kain dengan teknik pewarnaan khusus yang berasal dari Indonesia.\n",
            "Setiap jenis batik memiliki keunikan dan nilai seni yang berbeda-beda, tergantung pada motif, warna, dan teknik pembuatannya.\n",
            "Selain itu, batik juga terus menjadi bagian penting dari budaya Indonesia dan dihargai oleh banyak orang di seluruh dunia.\n",
            "\n",
            "QUESTION: apa saja motif batik\n",
            "Setiap daerah di Indonesia memiliki motif batik yang khas dan unik.\n",
            "Beberapa contoh motif batik yang terkenal adalah Batik Parang dari Yogyakarta, Batik Lasem dari Jawa Tengah, dan Batik Tulis dari Solo.\n",
            "Batik Tulis - merupakan jenis batik yang dibuat secara tradisional dengan teknik menulis atau menorehkan motif pada kain menggunakan lilin cair.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "\n",
        "question=[\"apa itu seni batik\",\"apa saja motif batik\"]\n",
        "\n",
        "for j in question:\n",
        "\n",
        " \n",
        "  qq=[]\n",
        "  qp=[]\n",
        "  que=sent_tokenize(j)\n",
        "  \n",
        "  qq.append(que)\n",
        "  qp.append(j)\n",
        "  questiontags = ['WP','WDT','WP$','WRB']\n",
        "  question_POS = pos_tag(word_tokenize(j.lower()))\n",
        "        \n",
        "  question_Tags=[]\n",
        "  for token in question_POS:\n",
        "      if token[1] in questiontags:\n",
        "          question_Tags.append(token)\n",
        "  \n",
        " \n",
        "  if len(question_Tags)>1:\n",
        "     if ' and ' in j :\n",
        "       pos=j.lower().find('and')\n",
        "       qq=[]\n",
        "       qp=[]\n",
        "       qp.append(j[:pos])\n",
        "       qp.append(j[pos+1:])\n",
        "       qq.append(sent_tokenize(j[:pos])) \n",
        "       qq.append(sent_tokenize(j[pos+1:])) \n",
        "  \n",
        "     \n",
        "               \n",
        "  \n",
        "\n",
        "  \n",
        "  print(\"QUESTION:\",j)\n",
        "  for k in range(len(qp)):   \n",
        "       \n",
        "\n",
        "\n",
        "    sentences=sent_tokenize(s)\n",
        "    #q contains a list of cleaned sentence tokens of question\n",
        "    q=get_cleaned_sentences(qq[k],stopwords=True)\n",
        "    #preprocessed contains a list of cleaned sentence tokens of the reference text\n",
        "    preprocessed=get_cleaned_sentences(sentences,stopwords=True)\n",
        "  \n",
        "    preprocessed.append(q[0])\n",
        "    i=len(preprocessed)-1\n",
        "    \n",
        "  \n",
        "    tfidf_vect=TfidfVectorizer()\n",
        "    df=create_document_term_matrix(preprocessed,tfidf_vect) \n",
        "    df_list = df.values.tolist()\n",
        "    answertype(qp[k])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HTgV5KvWgjaJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}